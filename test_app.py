# #!/usr/bin/env python3
# """
# Quick test script for Job Ad Analyzer
# """

# import logging
# import os
# import sys
# from pathlib import Path

# # Add src to path
# sys.path.insert(0, str(Path(__file__).parent / "src"))

# from src.scraper import WebScraper
# from src.llm_client import LLMClient
# from src.utils import setup_logging, create_sample_files
# import config


# def test_environment():
#     """Test environment setup"""
#     print("ğŸ” Testing environment setup...")
    
#     issues = []
    
#     # Check API key
#     if not config.LLM_API_KEY:
#         issues.append("âŒ OPENAI_API_KEY not set in environment")
#     else:
#         print("âœ… API key found")
    
#     # Check directories
#     try:
#         from src.utils import ensure_directories
#         ensure_directories()
#         print("âœ… Directory structure OK")
#     except Exception as e:
#         issues.append(f"âŒ Directory setup failed: {e}")
    
#     # Check sample files
#     if not config.URLS_FILE.exists():
#         issues.append("âŒ URLs file missing: data/input/urls.txt")
#     else:
#         print("âœ… URLs file found")
        
#     if not config.PROMPT_FILE.exists():
#         issues.append("âŒ Prompt file missing: data/input/master_prompt.txt") 
#     else:
#         print("âœ… Master prompt file found")
    
#     return issues


# def test_scraper():
#     """Test web scraper"""
#     print("\nğŸ•·ï¸ Testing web scraper...")
    
#     # Test with a real website
#     test_url = "https://httpbin.org/html"  # Simple test page
    
#     try:
#         scraper = WebScraper()
#         result = scraper.test_scraping(test_url)
        
#         if result["success"]:
#             print("âœ… Scraper working")
#             print(f"   Status: {result['status_code']}")
#             print(f"   Title: {result['title']}")
#             print(f"   Content length: {result['content_length']}")
#         else:
#             print(f"âŒ Scraper failed: {result['error']}")
#             return False
            
#     except Exception as e:
#         print(f"âŒ Scraper test failed: {e}")
#         return False
    
#     return True


# def test_llm_client():
#     """Test LLM client"""
#     print("\nğŸ¤– Testing LLM client...")
    
#     if not config.LLM_API_KEY:
#         print("âŒ Cannot test LLM - no API key")
#         return False
    
#     try:
#         client = LLMClient()
        
#         # Test connection
#         if client.test_connection():
#             print("âœ… LLM connection working")
            
#             # Test with sample job ad
#             sample_content = """
#             Software Engineer Position
            
#             We are looking for a skilled Software Engineer to join our team.
            
#             Requirements:
#             - 3+ years of Python experience
#             - Experience with web frameworks
#             - Bachelor's degree in Computer Science
            
#             Location: San Francisco, CA
#             Salary: $100,000 - $130,000
#             Type: Full-time
#             """
            
#             sample_prompt = """Extract job information as JSON:
#             {
#                 "job_title": "title",
#                 "location": "location", 
#                 "salary_min": 0,
#                 "salary_max": 0
#             }"""
            
#             response = client.analyze_job_ad(sample_content, sample_prompt, "test")
            
#             if response:
#                 print("âœ… LLM analysis working")
#                 print(f"   Sample response: {response}")
#             else:
#                 print("âŒ LLM analysis failed")
#                 return False
                
#         else:
#             print("âŒ LLM connection failed")
#             return False
            
#     except Exception as e:
#         print(f"âŒ LLM test failed: {e}")
#         return False
    
#     return True


# def run_mini_pipeline():
#     """Run a mini version of the pipeline"""
#     print("\nğŸš€ Running mini pipeline test...")
    
#     try:
#         # Use httpbin for a predictable test
#         test_urls = ["https://httpbin.org/html"]
        
#         # Create test prompt
#         test_prompt = """Please analyze this content and return JSON:
#         {"content_type": "description of content", "success": true}"""
        
#         scraper = WebScraper()
#         llm_client = LLMClient()
        
#         for i, url in enumerate(test_urls):
#             print(f"Processing test URL {i+1}: {url}")
            
#             # Scrape
#             content = scraper.scrape_url(url, f"test_{i+1}")
#             if not content:
#                 print("âŒ Scraping failed")
#                 continue
                
#             print(f"âœ… Scraped {len(content)} characters")
            
#             # Analyze with LLM
#             if config.LLM_API_KEY:
#                 response = llm_client.analyze_job_ad(content, test_prompt, f"test_{i+1}")
#                 if response:
#                     print(f"âœ… LLM analysis successful: {response}")
#                 else:
#                     print("âŒ LLM analysis failed")
#             else:
#                 print("âš ï¸ Skipping LLM test - no API key")
        
#         print("âœ… Mini pipeline test completed")
#         return True
        
#     except Exception as e:
#         print(f"âŒ Mini pipeline failed: {e}")
#         return False


# def main():
#     """Run all tests"""
#     print("ğŸ§ª Job Ad Analyzer - Quick Test\n")
    
#     # Setup logging
#     setup_logging()
    
#     # Create sample files if they don't exist
#     create_sample_files()
    
#     # Run tests
#     all_passed = True
    
#     # Test environment
#     issues = test_environment()
#     if issues:
#         print("\nâŒ Environment issues:")
#         for issue in issues:
#             print(f"  {issue}")
#         all_passed = False
    
#     # Test scraper
#     if not test_scraper():
#         all_passed = False
    
#     # Test LLM (optional)
#     if config.LLM_API_KEY:
#         if not test_llm_client():
#             all_passed = False
#     else:
#         print("\nâš ï¸ Skipping LLM tests - set OPENAI_API_KEY to test")
    
#     # Run mini pipeline
#     if all_passed and config.LLM_API_KEY:
#         run_mini_pipeline()
    
#     # Summary
#     print(f"\n{'='*50}")
#     if all_passed:
#         print("ğŸ‰ All tests passed! Your app is ready to use.")
#         print("\nTo run the full pipeline:")
#         print("1. Add real job URLs to data/input/urls.txt") 
#         print("2. Run: python main.py")
#     else:
#         print("âŒ Some tests failed. Fix the issues above before running the full pipeline.")
    
#     print(f"{'='*50}")


# if __name__ == "__main__":
#     main()

#!/usr/bin/env python3
"""
Debug LLM responses to see what's going wrong
"""

import json
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

import config
from src.utils import setup_logging, load_text_file
from src.scraper import WebScraper
from src.llm_client import LLMClient


def debug_llm_response():
    """Debug what the LLM is actually returning"""
    
    setup_logging()
    
    # Check if we have a saved LLM response file
    response_files = list(config.PROCESSED_DATA_DIR.glob("*_llm_response.json"))
    
    if response_files:
        print("ğŸ” Found LLM response files:")
        for file in response_files:
            print(f"  - {file.name}")
        
        # Load the most recent one
        latest_file = max(response_files, key=lambda x: x.stat().st_mtime)
        print(f"\nğŸ“„ Analyzing: {latest_file.name}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                debug_data = json.load(f)
            
            print(f"\nğŸ“Š Response Details:")
            print(f"  URL ID: {debug_data.get('url_id')}")
            print(f"  Model: {debug_data.get('model')}")
            print(f"  Prompt Length: {debug_data.get('prompt_length')} chars")
            print(f"  Response Length: {debug_data.get('response_length')} chars")
            
            raw_response = debug_data.get('raw_response', '')
            print(f"\nğŸ“ Raw LLM Response:")
            print("="*60)
            print(raw_response)
            print("="*60)
            
            # Try to find JSON manually
            json_start = raw_response.find('{')
            json_end = raw_response.rfind('}')
            
            if json_start != -1 and json_end != -1:
                potential_json = raw_response[json_start:json_end + 1]
                print(f"\nğŸ”§ Extracted JSON (chars {json_start}-{json_end}):")
                print("-"*40)
                print(potential_json)
                print("-"*40)
                
                # Try to parse it
                try:
                    parsed = json.loads(potential_json)
                    print("\nâœ… JSON is valid!")
                    print("ğŸ—‚ï¸ Keys found:", list(parsed.keys()))
                except json.JSONDecodeError as e:
                    print(f"\nâŒ JSON parsing failed: {e}")
                    print("ğŸ”§ Trying to fix common issues...")
                    
                    # Try basic fixes
                    fixed_json = fix_json_issues(potential_json)
                    try:
                        parsed = json.loads(fixed_json)
                        print("âœ… Fixed JSON is valid!")
                        print("ğŸ—‚ï¸ Keys found:", list(parsed.keys()))
                        return True
                    except json.JSONDecodeError as e2:
                        print(f"âŒ Still couldn't fix JSON: {e2}")
                        return False
            else:
                print("\nâŒ No JSON brackets found in response")
                print("ğŸ¤” The LLM might be returning explanation text instead of JSON")
                return False
                
        except Exception as e:
            print(f"âŒ Error reading debug file: {e}")
            return False
    
    else:
        print("âŒ No LLM response debug files found")
        print("ğŸ”§ Let's test the LLM directly...")
        return test_llm_directly()


def fix_json_issues(json_text):
    """Try to fix common JSON issues"""
    import re
    
    # Remove trailing commas
    json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)
    
    # Fix unquoted keys (basic attempt)
    json_text = re.sub(r'(\w+):', r'"\1":', json_text)
    
    # Fix single quotes to double quotes
    json_text = json_text.replace("'", '"')
    
    return json_text


def test_llm_directly():
    """Test LLM with a simple request"""
    
    print("\nğŸ§ª Testing LLM directly...")
    
    try:
        llm_client = LLMClient()
        
        # Simple test
        simple_prompt = """Please respond with ONLY this JSON and nothing else:
{
    "test": "success",
    "message": "This is a test response"
}"""
        
        print("ğŸ“¤ Sending simple test prompt...")
        
        from langchain_core.messages import HumanMessage
        response = llm_client.client.invoke([HumanMessage(content=simple_prompt)])
        
        print(f"\nğŸ“¥ LLM Response:")
        print("="*40)
        print(response.content)
        print("="*40)
        
        # Try to parse
        try:
            test_json = json.loads(response.content.strip())
            print("âœ… Simple JSON test passed!")
            return True
        except json.JSONDecodeError:
            print("âŒ LLM is not returning pure JSON")
            print("ğŸ”§ This suggests the LLM is adding extra text")
            return False
            
    except Exception as e:
        print(f"âŒ Direct LLM test failed: {e}")
        return False


def suggest_fixes():
    """Suggest potential fixes"""
    
    print("\nğŸ’¡ Potential Solutions:")
    print("="*50)
    
    print("1. ğŸ¯ PROMPT ISSUE:")
    print("   - Your prompt might be too complex")
    print("   - Try adding: 'RESPOND WITH ONLY JSON, NO OTHER TEXT'")
    print("   - Or: 'Do not include any explanations or text outside the JSON'")
    
    print("\n2. ğŸ¤– MODEL ISSUE:")
    print("   - Some models ignore JSON-only instructions")
    print("   - Try a different model (gpt-4, gpt-3.5-turbo)")
    print("   - Check if your API endpoint supports structured output")
    
    print("\n3. ğŸ”§ PARSING ISSUE:")
    print("   - The LLM might be returning valid JSON with extra text")
    print("   - We can improve the JSON extraction logic")
    
    print("\n4. ğŸŒ API ISSUE:")
    print("   - Check if you're using a custom API endpoint")
    print("   - Verify your API key and model access")
    
    print("\nğŸš€ Quick Fix: Try running with a simpler prompt first!")


def main():
    """Main debug function"""
    
    print("ğŸ› LLM Response Debugger\n")
    
    success = debug_llm_response()
    
    if not success:
        suggest_fixes()
    
    print(f"\n{'='*60}")


if __name__ == "__main__":
    main()